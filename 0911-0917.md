
* * *

### Do Deep Nets Really Need to be Deep？

1. 论文基于 2 中的模型压缩技术，从实验角度去说明浅层网络也可以达到深层网络一样甚至更好的结果
2. 但它并不是直接基于原始训练数据训练一个浅层网络，而是先训练出一个 state of art 的深度网络，再用浅层网络去逼近深度网络（对于浅层网络的输入来说，其实是深度网络的输出数据，但不一定是最后一层结果）
3. 当然，本文只是从实验角度去说明这个问题，并没有从理论上分析

* * *

### Model Compression（KDD06）

1. 这是 06 年的[一篇论文](https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf)，由 1 中链接而来，其中的方法比较 general，所以值得一读
2. 输入是高复杂度模型的输入数据，因为模型已知，所以对于简单网络来说，其输入数据就非常丰富了，且这些数据不会造成 overfit
3. 论文的另一个创新点在于，提出了一种生成无标签数据的方法 **MUNGE**

* * *

### Recent Advances in Convolutional Neural Networks  

1. [本文](https://arxiv.org/abs/1512.07108)发表于 2015 年，几大主流 CNN 框架也已经提出，本文主要讨论了 CNN 的基础、各项优化等，非常值得一读
2. 按 CNN 的构成划分如下：
3. ![](imgs/cnn_components.jpg)
4. 注意，对于分类问题，最后一层一般使用 Softmax，但也可以使用其它操作，如 SVM 等分类方法
5. 对 CNN 的优化可以从如下 6 个方面进行：
    1. 卷积层：直接从卷积结构上改造，可以支持不同的应用（如反卷积操作，就可以作用于视觉中不少问题，如 SR）[这里再特别注意 Inception Module]
    2. Pooling 层
    3. 激活函数：ReLU 在实践中优于 sigmoid & tanh 函数，也有不少对 ReLU 的改进工作
        1. ![](imgs/relu_varients.jpg)
        2. 另一个值得注意的是 Maxout 函数
    4. Loss 函数
    5. 正则化
        1. Lp 正则化
        2. Dropout：提高网络的泛化能力
    6. 优化方法（这里不仅仅只是数值优化 SGD 等的改进）
        1. 数据增强：在图像相关的算法上，可以通过增加随机旋转、平移、镜面翻转等手段，提高训练集多样性
        2. 参数初始化（很重要）：参数的选择不合理，可能会导致梯度爆炸或梯度消失问题，这里重点提到了 Xavier 初始化技术（知乎上也有对它的一个分析 https://zhuanlan.zhihu.com/p/22028079）
        3. SGD
        4. Batch Normalization：一般情况下，Normalization 用于对数据进行预处理，以保证输入数据符合一个标准的高斯分布，但随着数据在网络结构中的流动，中间层的数据极有可能会出现不标准分布（covariate shift problem），Batch Normalization 就是为了避免这种问题。
        5. Shortcut Connections：最近几年有很多这方面的工作，如 ResNet, HighwayNet 等，都是在网络中加入一个快速通路，直连某些层
6. CNN 相关应用（广泛应用于视觉及 NLP、文本分析等领域）

* * * 

### Image-to-Image Translation with Conditional Adversarial Networks（pix2pix）

<https://arxiv.org/abs/1611.07004>
